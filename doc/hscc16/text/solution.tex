\section{Solution}\label{sec:solution}
{\color{red} explain say why we call it framework}

\subsection{Decision Tree classifiers} \label{sec:dtree}
short description of decision tree classifiers 

\subsection{Algorithm}
\label{sec:alg}

In Alg.~\ref{alg:inf} we present a parameterized procedure
for inferring temporal logic formulae from data.
The algorithm is recursive and takes as input arguments the
parent node, the set of data to classify and
the current depth level, and returns
a binary decision tree which classifies the data.
The parameters of Alg.~\ref{alg:inf} are:
(1) the set of primitive STL formulae $\CA{P}$;
(2) the impurity measure $J$; and
(3) the stop criteria $stop$.

\begin{algorithm}
\caption{Temporal Inference -- $Tree(\cdot)$}
\label{alg:inf}
\DontPrintSemicolon
\KwIn{$pa$ -- parent of the current node}
\KwIn{$S=\{(s^i, l^i)_{i=1}^N\}$ -- set of labeled signals}
\KwIn{$h$ -- the current depth level}
\KwIn{$\CA{P}$ -- set of primitive PSTL formulae}
\KwIn{$J$ -- impurity measure}
\KwIn{$stop$ -- stop criterion}
\KwOut{a (sub)-tree}
\BlankLine

\If{$p_i = c$, $\forall i \in (1,\dots,N)$, $c\in C$}{
    \Return{$leaf(c)$}
}
\If{$S = \emptyset$}{
    \uIf{node is left child of $pa$}{
        \Return{$leaf(C_p)$}
    }
    \Else{
        \Return{$leaf(C_n)$}
    }
}
$\phi^* = \argmax_{\psi \in \CA{P}, \theta \in \Theta} J(S, partition(S, \phi_\theta \andltl pa.\phi))$\;
$t \asgn non\_terminal(\phi^*)$\;
$S^*_\top, S^*_\perp \asgn J(S, partition(S, \phi^* \andltl pa.\phi))$\;
\eIf{$stop(pa, h, S, \phi^*)$}{
  $t.left \asgn leaf(\argmax_{c\in C} \{ p(S^*_\top, c; \phi^*) \} )$\;
  $t.right \asgn leaf(\argmax_{c\in C} \{ p(S^*_\perp, c; \phi^*) \} )$\;
}{
  $t.left \asgn Tree(t, S^*_\top, h+1)$\;
  $t.right \asgn Tree(t, S^*_\perp, h+1)$\;
}
\Return{$t$}
\end{algorithm}

The procedure presented in Alg.~\ref{alg:inf}
recursively constructs a decision tree form a
set of labeled signals $S$.
If all the signals belong to a single class $c\in C$ (line 1),
either positive or negative, then the
algorithm returns a single leaf node marked
with the class label $c$ (line 2).
In case the set of signals $S$ is empty (line 3),
a leaf node is returned as well marked with
the positive or negative class label if the node
is a right or left child of the parent node $pa$
(lines 4-7), respectively.
If none of the two corner cases are encountered,
then the algorithm proceeds to find the optimal
STL formula among all the valuations of a PSTL
formula from the set of primitive formulae
$\CA{P}$ (line 8).
Cost function used in the optimization is the
impurity measure $J$, which assesses the quality
of the partition induced by a valuation of a
primitive PSTL formula form $\CA{P}$.
At line 9, a new non-terminal node is created
and associated with the optimal STL formula
$\phi^*$.
Next the partition induced by the optimal
formula $\phi^*$ is computed (line 10).
The stop condition is checked (line 11).
If the test is positive, then the left
and right children are assigned leaf
nodes marked with class labels corresponding
to the best quality classification (lines 12-13).
The quality is quantified by the
inter-partition weights, see Sec.~\ref{sec:impurity}
for more details.
Otherwise, if the test is negative, the $Tree()$
procedure is recursively called for each child
with the corresponding of signals from the
optimal partition and depth level increased by one
(lines 15-16).

The parameterized family of algorithms uses
three procedures: (a) $leaf(c)$ creates
a leaf node marked with the class label $c \in C$,
(b) $non\_terminal(\phi)$ creates an
intermediate node associated with the
valuation of a primitive PSTL formula from $\CA{P}$,
and (c) $partition(S, \phi)$ splits the set
of signals $S$ into satisfying and non-satisfying
signals with respect to $\phi$, i.e.
$S_\top, S_\perp = partition(S, \phi)$, where
$S_\top = \{(s^i, l^i) \in S \ |\ s^i \models \phi \}$
and
$S_\perp = \{(s^i, l^i) \in S \ |\ s^i \not\models \phi \}$.

Given a set of labeled signals $S_{root}$, a
decision tree is obtained by executing
$Tree(\emptyset, S_{root}, 0)$.
The returned tree constructed from $S_{root}$
depends on the particular parameters ($\CA{P}$,
$J$ and $stop$) used.
In Sec.~\ref{sec:alg-instances}, we discuss
a few particular instances of the $Tree()$
procedure.

Lastly, an STL formula can be obtained from
a decision tree using Alg.~\ref{alg:tree2formula}.
The algorithm recursively traverses the tree
given as parameter and constructs an STL
formulae for each branch ending in a leaf node
marked with the positive class label $C_p$ (lines 5-6).
All the formulae along such a branch are connected
by conjunction (lines 8-9). Finally, all formulae
obtained from the branches are connected by
disjunction (line 10).

\begin{algorithm}
\caption{Tree to formula -- $Tree2STL(\cdot)$}
\label{alg:tree2formula}
\DontPrintSemicolon
\KwIn{$tree$ -- parent of the current node}
\KwOut{formula}
\BlankLine

$stack \asgn \{(tree, tree.\phi)\}$\;
$\Phi \asgn \emptyset$\;
\While{$stack \neq \emptyset$}{
    $t, \phi \asgn stack.pop()$\;
    \uIf{$t$ is leaf and $t.c = C_p$}{
        $\Phi \asgn \Phi \cup \{\phi\}$
    }
    \ElseIf{$t$ is non-terminal}{
        $stack.push((t.left, \phi \andltl t.\phi))$\;
        $stack.push((t.right, \phi \andltl \notltl t.\phi))$
    }
}

\Return{$\bigvee_{\phi \in \Phi} \phi$}
\end{algorithm}

\subsection{Primitives PSTL formulae}\label{sec:primitive}

In this paper, we define primitive formulae given in PSTL and the induced fragments of STL by Boolean closure.
In this paper, we consider two particular sets of primitives which are used in the algorithms presented in Sec.~\ref{sec:solution}.

\begin{definition}[Boolean Closure]
\label{def:boolean-closure}
Let $\CA{P}$ be a finite set of PSTL formulae.
The fragment of STL formulae induced by $\CA{P}$ using Boolean closure is defines as:
\begin{equation*}
\label{eq:boolean-closure}
\phi ::= \True \ |\ \varphi \ |\ \notltl\phi \ |\ \phi_1 \andltl \phi_2 \ |\ \phi_1 \orltl \phi_2
\end{equation*}
where $\varphi$ is a valuation of a PSTL formula from $\CA{P}$.
\end{definition}

Given a set of primitives $\CA{P}$, we denote by STL$_\CA{P}$ the STL fragment obtained by Boolean closure from $\CA{P}$.


In the following, we define two particular sets of primitives.

\begin{definition}[First-Order Primitives]
\label{def:first-order}
Let $\CA{S}$ be the set of signals with values in $\BB{R}^n$, $n \geq 1$.
We define the set of $1^{st}$ order primitive as follows:
\begin{align*}
\CA{P}_1 =\ & \big\{\Event_{[\tau_1, \tau_2)} (s_i \sim \mu) \text{ or } \Always_{[\tau_1, \tau_2)} (s_i \sim \mu)  \ \big|\ \tau_1 < \tau_2\in \BB{R}_{\geq 0},\\
                 &\qquad \mu \in \BB{R}, i\in \overline{(1,n)}, \sim \in \{\leq, >\} \big\}
\end{align*}
The space of parameters of $\CA{P}_1$ is $\Theta_1 = [\mu^L, \mu^U] \times \{(\tau_1, \tau_2)\ |\ \tau^L \leq \tau_1 \leq \tau_2 \leq \tau^U\}$,
where $\mu^L, \mu^U \in \BB{R}$ and $\tau^L, \tau^U \in \BB{R}_{\geq 0}$.
\end{definition}

\begin{definition}[Second-Order Primitives]
\label{def:second-order}
Let $\CA{S}$ be the set of signals with values in $\BB{R}^n$, $n \geq 1$.
We define the set of $2^{nd}$ order primitive as follows:
\begin{align*}
\CA{P}_2 =\ & \big\{\Always_{[\tau_1, \tau_2)}\Event_{[0, \tau_3)} (s_i \sim \mu) \text{ or } \Event_{[\tau_1, \tau_2)}\Always_{[0, \tau_3)} (s_i \sim \mu)  \ \big|\\
               &\ \tau_1 < \tau_2 \in \BB{R}_{\geq 0}, \tau_3 \in \BB{R}_{\geq 0}, \mu \in \BB{R}, i\in \overline{(1,n)}, \sim \in \{\leq, >\} \big\}
\end{align*}
The space of parameters of $\CA{P}_2$ is $\Theta_2 = [\mu^L, \mu^U] \times \{(\tau_1, \tau_2)\ |\ \tau^L_1 \leq \tau_1 \leq \tau_2 \leq \tau^U_2\} \times [0, \tau^U_3]$, where $\mu^L, \mu^U \in \BB{R}$ and $\tau^L_1, \tau^U_2, \tau^U_3 \in \BB{R}_{\geq 0}$.
\end{definition}

Note that $\text{STL}_{\CA{P}_1} \subset \text{STL}_{\CA{P}_2}$, because $\Event_{[\tau_1, \tau_2)} l = \Event_{[\tau_1, \tau_2)} \Always_{[0, 0)} l$
and similarly $\Always_{[\tau_1, \tau_2)} l = \Always_{[\tau_1, \tau_2)} \Event_{[0, 0)} l$, where $l \equiv (s_i \sim \mu)$ is a linear inequality predicate.


{\color{orange}
TODO:\\
simple examples of properties\\
%comparison with full STL, rSTL and iSTL (and parametric variants)\\
}


\subsection{Impurity Measures and Local Optimization}
\label{sec:impurity}

In this section, we review some popular impurity measures,
e.g. information gain, Gini index and misclassification rate.
Additionally, we propose extensions to of the usual definitions
of these measures motivated by Prop.~\ref{th:robustness-relative}.
The result in Prop.~\ref{th:robustness-relative} can be used
to interpret robustness degree in the context of learning as a
measure of quality of classification of a signal with respect to
an STL formula. This observation forms the basis of the
extensions presented in this section.

\begin{definition}[Impurity Measures]
\label{def:impurity}
Let $S$ be a finite set of signals, $\phi$ an STL formula and
$S_\top, S_\perp = partition(S, \phi)$. We have the
following impurity measures:
\begin{itemize}
  \item {\it Ingormation gain (IG)}
  {\scriptsize
  \begin{align}
  IG(S, \{S_\top, S_\perp\}) &= H(S) - \sum_{\otimes \in \{\top, \perp\}}p_\otimes\cdot H(S_\otimes)\\
  H(S) &= -\sum_{c \in C} p(S, c; \phi) \log p(S, c; \phi)
  \end{align}}%
  \item {\it Gini gain (G)}
  {\scriptsize
  \begin{align}
  G(S, \{S_\top, S_\perp\}) &= Gini(S) - \sum_{\otimes \in \{\top, \perp\}}p_\otimes\cdot Gini(S_\otimes)\\
  Gini(S) &= \sum_{c \in C} p(S, c; \phi) \big(1 - p(S, c; \phi)\big)
  \end{align}}%
  \item {\it Misclassification gain (MG)}
  {\scriptsize
  \begin{align}
  MG(S, \{S_\top, S_\perp\}) &= 1 - \sum_{\otimes \in \{\top, \perp\}}p_\otimes\cdot MR(S_\otimes,)\\
  MR(S) &= \begin{cases}
    p(S, C_n; \phi) & \text{if } s\models \phi, \forall s\in S\\
    p(S, C_p; \phi) & \text{if } s\not\models \phi, \forall s\in S
  \end{cases}
  \end{align}}%
\end{itemize}
where $p_\top = \frac{\card{S_\top}}{\card{S}}$ and
$p_\perp = \frac{\card{S_\perp}}{\card{S}}$ are the
intra-partition probabilities or weights,
$p(S, c;\phi)=\frac{\card{\{(s^i, l^i)\ |\ l^i=c \}}}{\card{S}}$
are the inter-partition probabilities or weights and $c \in C$.
\end{definition}

In the following, we extend the impurity measures
to account for the robustness degree of the signals
to be classified.
% Prop.~\ref{th:robustness-relative} justifies the new definitions of the impurity measures.

\begin{definition}[Extended Impurity Measures]
\label{def:impurity-ext}
Consider the same setup as in Def.~\ref{def:impurity}
and the same impurity measures, where we redefine
the intra- and inter-partition weight as follows:
\begin{align}
p_\top &= \frac{\sum_{s_i \in S_\top} r(s^i, \phi)}{\sum_{s_i \in S} \abs{r(s^i, \phi)}}\\
p_\perp &= -\frac{\sum_{s_i \in S_\perp} r(s^i, \phi)}{\sum_{s_i \in S} \abs{r(s^i, \phi)}}\\
p(S, c;\phi) &=  \frac{\sum_{s_i \in S_c} \abs{r(s^i, \phi)}}{\sum_{s_i \in S} \abs{r(s^i, \phi)}}
\end{align}
where $S_c = \{ s^i  \in S\ |\ l^i = c \}$.
\end{definition}

\begin{proposition}
The intra-partition weights are bound within $0$ and $1$ and sum to $1$, i.e. $0 \leq p_\top,  p_\perp \leq 1$ and $p_\top + p_\perp = 1$, in both definitions Def.~\ref{def:impurity} and Def.~\ref{def:impurity-ext}.
The same invariant property is true for the inter-partition weights, i.e. $0 \leq p(S, C_n; \phi), p(S, C_p; \phi) \leq 1$ and $\sum_{c\in C} p(S, c; \phi) = 1$.
\end{proposition}

In the following, we will distinguish
between the usual impurity measures and the
extended one by using the subscript $r$ (e.g. $IG_r$)
for the extended impurity measures.

\subsubsection{Local Optimization}

Given an impurity measure $J$, Alg.~\ref{alg:inf}
solves an optimization problem at line 8 for
each intermediate node with respect to $J$.
The optimization is performed over the
chosen set of primitive PSTL set $\CA{P}$
and their valuations. Thus, the optimization
problem can be decomposed into $\card{\CA{P}}$
optimization problems over a fixed (and small)
number of real-valued parameters.

Consider signals of dimension $n$.
In the case of $\CA{P}_1$, we have $2n$
optimization problems each with 3 parameters.
On the other hand for $\CA{P}_2$, we have
$4n$ optimization problems each with 4
parameters.

One important aspect of the computation is
given by the following result which simplifies
the computation of robustness.

\begin{proposition}
{\color{orange}
TODO: IMPORTANT -- show incremental computation of robustness using recursive definition
}
Let $pa$ be a parent node, $S$ a set of
labeled signals. Then the robustness of
a signal $s \in S$ with respect to the
formula associated with the current
tree (including the current node)
depends on on the robustness computed
at the parent node $pa$ and the current
node
r()
\end{proposition}


\subsection{Stop Conditions}
\label{sec:stop-condition}

{\color{orange}
TODO: add description and examples of stop conditions:\\
$stop: TreeNode \times \BB{Z}_{\geq 0} \times \spow{\CA{S}} \times \text{STL}_\CA{P} \to \{\top, \perp\} $\\
none (identically false), depth based, performance based
(either on the value of $J$ or on the induced optimal partition)
with either bad performance or good enough performance criteria.
}

\subsection{Instantiations of the algorithm}
\label{sec:alg-instances}

{\color{orange}
set algorithm parameters -- obtain algorithm
to construct classification tree -- STL formula
}

\subsection{Analysis}
\label{sec:analysis}

TODO: (1) show that algorithm terminates; (2) provide complexity bound.

\subsection{Comparison to previous result}
\label{sec:comp-pSTL}

Advantages:
\begin{itemize}
    \item incremental computation: (1) formula; and (2) robustness;
    \item optimal valuation problem becomes easier and easier, because the number of parameters is always 4
    and the number of instances/trajectories/signals to consider decreases at each iteration;
    \item algorithm will always terminate.
\end{itemize}
